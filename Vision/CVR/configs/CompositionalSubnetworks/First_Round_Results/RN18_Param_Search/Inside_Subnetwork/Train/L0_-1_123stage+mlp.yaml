path_db: '/gpfs/data/epavlick/mlepori/projects/Compositional_Subnetworks/Models/resnet18/Param_Sweep/Inside'
model: 'CNN'
dataset: 'CVRTDataModule'
# trainer
gpus: 1
max_epochs: 120
num_workers: 1
checkpoint: ''
finetune: 0
freeze_pretrained: 0
log_every_n_steps: 100
ckpt_period: 1
seed: -1
early_stopping: 0
refresh_rate: 10
es_patience: 20
check_val_every_n_epoch: 1
#arch
backbone: 'L0resnet18'
lr: 0.0001
wd: 0.00
n_tasks: 0
mlp_dim: 128
mlp_hidden_dim: 256
task_embedding: 0
# data
data_dir: '/users/mlepori/data/mlepori/projects/Compositional_Subnetworks/Data/'
task: '104' # Denotes multitask training
batch_size: 64
train_transform: null
val_transform: null
n_samples: 10000 # 5000, 1000
test_set: ''
exp_name: "L0_-1_stage123+mlp"
exp_dir: '/gpfs/data/epavlick/mlepori/projects/Compositional_Subnetworks/Models/resnet18/Param_Sweep/Inside/L0_-1_stage123+mlp'

# Config variables introduced for mask identification
l0_components: {"backbone": True, "mlp": True}
train_masks: {"backbone": True, "mlp": True}
train_weights: {"backbone": False, "mlp": False, "embedding": False}
pretrained_weights: { 
  "backbone": '/gpfs/data/epavlick/mlepori/projects/Compositional_Subnetworks/Models/resnet18/Comp_Training/0/backbone.pt',
  "mlp": '/gpfs/data/epavlick/mlepori/projects/Compositional_Subnetworks/Models/resnet18/Comp_Training/0/mlp.pt',
  "embedding": False
  } # False for random initialization
l0_init: -0.1
l0_lambda: .00000001
max_temp: 300
eval_only: False # This is derivable from train_masks and train_weights, but make it explicit with another variable
l0_stages: ["stage_1", "stage_2", "stage_3"]

